<!DOCTYPE HTML>
<html lang="en">
  
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Kechun Xu</title>
  
  <meta name="author" content="Kechun Xu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <!-- <link rel="icon" type="image/png" href="images/zju_icon.png"> -->
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Kechun Xu</name>
              </p>
              <p>I am a final-year PhD (Sep. 2021 - ) student in Control Science and Engineering department at <a href="https://www.zju.edu.cn/english/">Zhejiang University</a>. I belong to Robotics Lab</a>, advised by Prof. <a href="https://person.zju.edu.cn/en/rongxiong">Rong Xiong</a> and <a href="https://ywang-zju.github.io/">Yue Wang</a>. I was a visiting student at <a href="https://msc.berkeley.edu/">MSC Lab</a> in <a href="https://www.berkeley.edu/"></a>UC Berkeley</a>, advised by Prof. <a href="https://me.berkeley.edu/people/masayoshi-tomizuka/">Masayoshi Tomizuka</a>. I also spent time in <a href="">Alibaba Cloud</a> and <a href="https://www.zj-humanoid.com/">ZJ Humanoid</a> as a research intern.
              </p>
              <p>
                I obtained my B.Eng (Sep. 2017 - Jun. 2021) in Control Science and Engineering and Mechanical and Electronic Engineering from <a href="https://www.zju.edu.cn/english/">Zhejiang University</a> with an honor degree at Chu Kochen Honor College.
              </p>
              <p>
                My current research interests lie in robotic manipulation, robot learning, and embodied AI. My research goal is to develop human-level intelligence in robotic manipulation, including behavior dexterity, learning efficiency, and open-world generalization. 
              </p>
              <p style="color: red;">
                I am now open to potential academic and industry job opportunities. Please feel free to contact me! 
              </p>
              <p style="text-align:center">
                <a href="mailto:kcxu@zju.edu.cn">Email</a> &nbsp/&nbsp
                <!-- <a href="data/CV.pdf">CV</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?hl=en&user=WWsR278AAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/xukechun">Github</a> &nbsp/&nbsp
                <a href="https://twitter.com/KechunXu">Twitter</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/kechun-xu-12aaab1a7/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/Kechun_Xu.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Kechun_Xu.jpg" class="hoverZoomLink"></a>
              <p style="text-align:center">
                Photo taken by <a href="https://kyleleey.github.io/">Zizhang Li</a>.
              </p>
            </td>
          </tr>
        </tbody></table>

        <hr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:10px;width:100%;vertical-align:middle">
                  <heading>Recent News</heading>
              </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>       
          <tr>
            <ul>
              <li>[01/2026] <a href="https://arxiv.org/abs/2503.09423">A2</a> and <a href="https://hz-du.github.io/data/PolyFold.pdf">E2VLA</a> accepted to <b>ICRA 2026</b>.</li>
            </ul>
          </td>
        </tr>
          <tr>
            <ul>
              <li>[01/2026] One paper <a href="https://hz-du.github.io/data/PolyFold.pdf">PolyFold</a> accepted to <b>TASE 2026</b>.</li>
            </ul>
          </td>
        </tr>
          <tr>
            <ul>
              <li>[08/2025] One paper <a href="https://arxiv.org/abs/2503.09423">A2</a> accepted to <b>TASE 2025</b>.</li>
            </ul>
          </td>
        </tr>
          <tr>
            <ul>
              <li>[05/2025] Invited talk at <a href="https://www.bilibili.com/video/BV1WRjCzpESD/">3D视觉工坊</a>.</li>
            </ul>
          </td>
        </tr>
          <tr>
            <ul>
              <li>[04/2025] Invited talk at <a href="https://www.shenlanxueyuan.com/open/course/273">Autonomous Robotic Technology Seminar (ARTS)</a>.</li>
            </ul>
          </td>
        </tr>
        <tr>
          <ul>
            <li>[12/2024] Invited talk at <a href="https://github.com/TEA-Lab">TEA Lab</a>, Tsinghua University.</li>
          </ul>
        </td>
      </tr>
        <tr>
          <ul>
            <li>[11/2024] Honored to receive <b>ARTS Scholarship (Top 5 in China)</b>.</li>
          </ul>
        </td>
      </tr>
          <tr>
            <ul>
              <li>[10/2024] One paper <a href="https://ieeexplore.ieee.org/document/10757428">GSP</a> accepted to <b>TRO 2024</b>.</li>
            </ul>
          </td>
        </tr>
        </tbody></table>
        
        <hr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:10px;width:100%;vertical-align:middle">
            <heading>Publications</heading>
          </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <!-- <ul>
            My current research interests lie in manipulation, robot learning, and embodied AI.
          </ul> -->
          <!-- <ul>
            * indicates equal contributions.
          </ul> -->
        </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/2025BayesVLA_xu.gif' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2512.11218">
                    <papertitle>Seeing to Act, Prompting to Specify: A Bayesian Factorization of Vision Language Action Policy</papertitle>
                </a>
                <br>
                <strong>Kechun Xu</strong>,
                <a>Zhenjie Zhu</a>,
                <a>Anzhe Chen</a>,
                <a>Shuqi Zhao</a>,
                <a>Qing Huang</a>,
                <a>Yifei Yang</a>,
                <a href="https://scholar.google.com/citations?user=dNAbVgIAAAAJ&hl=en">Haojian Lu</a>,
                <a href="https://scholar.google.com/citations?user=1hI9bqUAAAAJ&hl=en">Rong Xiong</a>,
                <a href="https://msc.berkeley.edu/people/tomizuka.html">Masayoshi Tomizuka</a>,
                <a href="https://ywang-zju.github.io/">Yue Wang</a>
                <br>
                <em>Under Review</em>
                </p>
                <div class="paper" id="Xu2025BayesVLA">
                  <a href="https://arxiv.org/abs/2512.11218">arXiv</a> /
                  <a href="https://xukechun.github.io/papers/BayesVLA">project</a> /
                  <a href="https://github.com/xukechun/BayesVLA">code</a> /
                  <a href="https://www.bilibili.com/video/BV1eA63BHExy/">video</a>
                </div>
            </td>
        </tr> <!--xu2025bayesvla--> <!--BayesVLA-->
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/2025E2VLA_chen.gif' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2509.14630">
                    <papertitle>Toward Embodiment Equivariant Vision-Language-Action Policy</papertitle>
                </a>
                <br>
                <a>Anzhe Chen</a>,
                <a>Yifei Yang</a>,
                <a>Zhenjie Zhu</a>,
                <strong>Kechun Xu</strong>,
                <a>Zhongxiang Zhou</a>,
                <a href="https://person.zju.edu.cn/en/rongxiong">Rong Xiong</a>,
                <a href="https://ywang-zju.github.io/">Yue Wang</a>
                <br>
                <em>IEEE International Conference on Robotics and Automation (ICRA 2026)</em>
                </p>
                <div class="paper" id="Chen2025E2VLA">
                  <a href="https://arxiv.org/abs/2509.14630">arXiv</a> /
                  <a href="https://github.com/hhcaz/e2vla">code</a> 
                </div>
            </td>
        </tr> <!--chen2025e2vla-->
        <!-- <tr>
          <td style="padding:20px;width:35%;vertical-align:middle">
              <img src='images/2025ExploreVLM_lou.gif' width="250"></div>
                  </td>
          <td width="75%" valign="middle">
              <p>
              <a href="https://arxiv.org/pdf/2508.11918">
                  <papertitle>ExploreVLM: Closed-Loop Robot Exploration Task Planning with Vision-Language Models</papertitle>
              </a>
              <br>
              <a>Zhichen Lou</a>,
              <strong>Kechun Xu</strong>,
              <a>Zhongxiang Zhou</a>,
              <a href="https://person.zju.edu.cn/en/rongxiong">Rong Xiong</a>
              <br>
              <em>Under Review</em>
              </p>
              <div class="paper" id="Lou2025ExploreVLM">
                <a href="https://arxiv.org/pdf/2508.11918">arXiv</a>
              </div>
          </td>
      </tr> lou2025explorevlm -->
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/2025A2C_xu.gif' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/abs/2503.09423">
                    <papertitle>Efficient Alignment of Unconditioned Action Prior for Language-conditioned Pick and Place in Clutter</papertitle>
                </a>
                <br>
                <strong>Kechun Xu</strong>,
                <a>Xunlong Xia</a>,
                <a>Kaixuan Wang</a>,
                <a>Yifei Yang</a>,
                <a href="https://yunxuanmao.github.io/">Yunxuan Mao</a>,
                <a>Bing Deng</a>,
                <a href="https://scholar.google.com/citations?user=T9AzhwcAAAAJ&hl=en&oi=ao">Jieping Ye</a>,
                <a href="https://person.zju.edu.cn/en/rongxiong">Rong Xiong</a>,
                <a href="https://ywang-zju.github.io/">Yue Wang</a>
                <br>
                <em>IEEE Transactions on Automation Science and Engineering (T-ASE 2025)</em>
                <br>
                <em>Conference on Robot Learning (CoRL 2025), GenPriors Workshop</em>
                <br>
                <em>IEEE International Conference on Robotics and Automation (ICRA 2026)</em>
                </p>
                <div class="paper" id="Xu2025A2">
                  <a href="https://arxiv.org/abs/2503.09423">arXiv</a> /
                  <a href="https://ieeexplore.ieee.org/document/11152358">ieee</a> /
                  <a href="https://xukechun.github.io/papers/A2">project</a> /
                  <a href="https://github.com/xukechun/Action-Prior-Alignment">code</a> /
                  <a href="https://www.bilibili.com/video/BV1dPX4YzEzk/?spm_id_from=333.1391.0.0">video</a>
                </div>
            </td>
        </tr> <!--xu2025a2-->
        <tr>
          <td style="padding:20px;width:35%;vertical-align:middle">
              <img src='images/2025UniMM_lin.gif' width="250"></div>
                  </td>
          <td width="75%" valign="middle">
              <p>
              <a href="https://arxiv.org/abs/2501.17015">
                  <papertitle>Revisit Mixture Models for Multi-Agent Simulation: Experimental Study within a Unified Framework</papertitle>
              </a>
              <br>
              <a>Longzhong Lin</a>,
              <a>Xuewu Lin</a>,
              <strong>Kechun Xu</strong>,
              <a href="https://person.zju.edu.cn/en/luhaojian">Haojian Lu</a>,
              <a>Lichao Huang</a>,
              <a href="https://person.zju.edu.cn/en/rongxiong">Rong Xiong</a>,
              <a href="https://ywang-zju.github.io/">Yue Wang</a>
              <br>
              <em>Under Review</em>
              </p>
              <div class="paper" id="Lin2025UmiMM">
                <a href="https://arxiv.org/abs/2501.17015">arXiv</a> /
                <a href="https://longzhong-lin.github.io/unimm-webpage/">project</a> /
                <a href="https://github.com/Longzhong-Lin/UniMM">code</a>
              </div>
          </td>
      </tr> <!--lin2025unimm-->
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/2025PolyFold_du.gif' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://hz-du.github.io/data/PolyFold.pdf">
                    <papertitle>PolyFold: A Generalizable Framework for Language-Conditioned Bimanual Cloth Folding</papertitle>
                </a>
                <br>
                <a href="https://hz-du.github.io/">Haozhe Du</a>,
                <strong>Kechun Xu</strong>,
                <a href="https://person.zju.edu.cn/en/rongxiong">Rong Xiong</a>,
                <a href="https://ywang-zju.github.io/">Yue Wang</a>
                <br>
                <em>IEEE Transactions on Automation Science and Engineering (T-ASE 2026)</em>
                </p>
                <div class="paper" id="Du2025PolyFold">
                  <a href="https://hz-du.github.io/data/PolyFold.pdf">arXiv</a> /
                  <a href="https://sites.google.com/view/polyfold">project</a> 
                </div>
            </td>
        </tr> <!--du2025polyfold-->
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
                <img src='images/2025AGPIL_zhu.png' width="250"></div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="">
                    <papertitle>Grounding 3D Object Affordance with Language Instructions, Visual Observations and Interactions</papertitle>
                </a>
                <br>
                <a>He Zhu</a>,
                <a href="https://qykong.github.io/">Quyu Kong</a>,
                <strong>Kechun Xu</strong>,
                <a>Xunlong Xia</a>,
                <a>Bing Deng</a>,
                <a href="http://www.yelabs.net/">Jieping Ye</a>,
                <a href="https://person.zju.edu.cn/en/rongxiong">Rong Xiong</a>,
                <a href="https://ywang-zju.github.io/">Yue Wang</a>
                <br>
                <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2025)</em>
                </p>
                <div class="paper" id="Zhu2025AGPIL">
                  <a href="https://arxiv.org/abs/2504.04744">arXiv</a> /
                  <a href="https://sites.google.com/view/lmaffordance3d">project</a> /
                  <a href="https://github.com/cn-hezhu/LMAffordance3D">code</a> /
                  <a href="https://drive.google.com/file/d/1G7yobBCQYhPqsDY6Im7ax94akoq9pcn1/view">dataset</a>
                </div>
            </td>
        </tr> <!--zhu2025agpil-->
          <tr>
              <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/2024GSP_xu.gif' width="250"></div>
                      </td>
              <td width="75%" valign="middle">
                  <p>
                  <a href="https://arxiv.org/abs/2402.15402">
                      <papertitle>Grasp, See, and Place: Efficient Unknown Object Rearrangement with Policy Structure Prior</papertitle>
                  </a>
                  <br>
                  <strong>Kechun Xu</strong>,
                  <a>Zhongxiang Zhou</a>,
                  <a>Jun Wu</a>,
                  <a href="https://person.zju.edu.cn/en/luhaojian">Haojian Lu</a>,
                  <a href="https://person.zju.edu.cn/en/rongxiong">Rong Xiong</a>,
                  <a href="https://ywang-zju.github.io/">Yue Wang</a>
                  <br>
                  <em>IEEE Transactions on Robotics (T-RO 2024)</em>
                  <br>
                  <em>IEEE International Conference on Robotics and Automation (ICRA 2025)</em>
                  </p>
                  <div class="paper" id="Xu2024GSP">
                    <a href="https://arxiv.org/abs/2402.15402">arXiv</a> /
                    <a href="https://ieeexplore.ieee.org/document/10757428">ieee</a> /
                    <a href="https://xukechun.github.io/papers/GSP">project</a> /
                    <a href="https://github.com/xukechun/GSP">code</a>
                  </div>
              </td>
          </tr> <!--xu2024gsp-->

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:20px;width:35%;vertical-align:middle">
                    <img src='images/2024NUVS_yu.gif' width="250"></div>
                        </td>
                <td width="75%" valign="middle">
                    <p>
                    <a href="https://ieeexplore.ieee.org/document/10610364">
                        <papertitle>Adapting for Calibration Disturbances: A Neural Uncalibrated Visual Servoing Policy</papertitle>
                    </a>
                    <br>
                    <a>Hongxiang Yu</a>,
                    <a>Anzhe Chen</a>,
                    <strong>Kechun Xu</strong>,
                    <a>Dashun Guo</a>,
                    <a>Yufei Wei</a>,
                    <a>Zhongxiang Zhou</a>,
                    <a>Xuebo Zhang</a>,
                    <a href="https://ywang-zju.github.io/">Yue Wang</a>,
                    <a href="https://person.zju.edu.cn/en/rongxiong">Rong Xiong</a>
                    <br>
                    <em>IEEE International Conference on Robotics and Automation (ICRA 2024)</em>
                    </p>
                    <div class="paper" id="Yu2023HPNNC">
                      <a href="https://ieeexplore.ieee.org/document/10610364">ieee</a> /
                      <a href="https://sites.google.com/view/neural-uncalibrated-vs">project</a>
                    </div>
                </td>
            </tr> <!--yu2024nuvs-->
            
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/2023HPN-NC_yu.gif' width="250"></div>
                      </td>
              <td width="75%" valign="middle">
                  <p>
                  <a href="https://arxiv.org/abs/2304.08952">
                      <papertitle>A Hyper-network Based End-to-end Visual Servoing with Arbitrary Desired Poses</papertitle>
                  </a>
                  <br>
                  <a>Hongxiang Yu</a>,
                  <a>Anzhe Chen</a>,
                  <strong>Kechun Xu</strong>,
                  <a>Zhongxiang Zhou</a>,
                  <a href="https://weijing.github.io/">Wei Jing</a>,
                  <a href="https://ywang-zju.github.io/">Yue Wang</a>,
                  <a href="https://person.zju.edu.cn/en/rongxiong">Rong Xiong</a>
                  <br>
                  <em>IEEE Robotics and Automation Letter (RA-L 2023)</em>
                  <br>
                  <em>IEEE International Conference on Robotics and Automation (ICRA 2024)</em>
                  </p>
                  <div class="paper" id="Yu2023HPNNC">
                    <a href="https://arxiv.org/abs/2304.08952">arXiv</a> /
                    <a href="https://ieeexplore.ieee.org/document/10158789">ieee</a>

                  </div>
              </td>
          </tr> <!--yu2023hpn-->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/2023OCFMLP_xu.png' width="250"></div>
                      </td>
              <td width="75%" valign="middle">
                  <p>
                  <a href="https://arxiv.org/abs/2304.02893">
                      <papertitle>Object-centric Inference for Language Conditioned Placement: A Foundation Model based Approach</papertitle>
                  </a>
                  <br>
                  <a href="https://ariszxxu.github.io/">Zhixuan Xu</a>,
                  <strong>Kechun Xu</strong>,
                  <a href="https://ywang-zju.github.io/">Yue Wang</a>,
                  <a href="https://person.zju.edu.cn/en/rongxiong">Rong Xiong</a>
                  <br>
                  <em>IEEE International Conference on Advanced Robotics and Mechatronics (ICARM 2023)</em>
                  </p>
                  <div class="paper" id="Xu2023OCFMLP">
                    <a href="https://arxiv.org/abs/2304.02893">arXiv</a> /
                    <a href="https://ieeexplore.ieee.org/document/10218865/">ieee</a>
                  </div>
              </td>
          </tr> <!--xu2023ocfmlp-->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/2023ViLG_xu.gif' width="250"></div>
                      </td>
              <td width="75%" valign="middle">
                  <p>
                  <a href="https://arxiv.org/abs/2302.12610">
                      <papertitle>A Joint Modeling of Vision-Language-Action for Target-oriented Grasping in Clutter</papertitle>
                  </a>
                  <br>
                  <strong>Kechun Xu</strong>,
                  <a>Shuqi Zhao</a>,
                  <a>Zhongxiang Zhou</a>,
                  <a href="https://kyleleey.github.io/">Zizhang Li</a>,
                  <a href="https://phj128.github.io/">Huaijin Pi</a>,
                  <a href="https://zhuyifengzju.github.io/">Yifeng Zhu</a>,
                  <a href="https://ywang-zju.github.io/">Yue Wang</a>,
                  <a href="https://person.zju.edu.cn/en/rongxiong">Rong Xiong</a>
                  <br>
                  <em>IEEE International Conference on Robotics and Automation (ICRA 2023)</em>
                  </p>
                  <div class="paper" id="Xu2023ViLG">
                    <a href="https://arxiv.org/abs/2302.12610">arXiv</a> /
                    <a href="https://ieeexplore.ieee.org/document/10161041">ieee</a> /
                    <a href="https://github.com/xukechun/Vision-Language-Grasping">code</a> /
                    <a href="https://www.bilibili.com/video/BV1yh4y1a7Ha/?spm_id_from=333.999.0.0">video</a>
                  </div>
              </td>
          </tr> <!--xu2023vilg-->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/2023FMP_xu.png' width="250"></div>
                      </td>
              <td width="75%" valign="middle">
                  <p>
                    <a href="https://arxiv.org/abs/2302.13024">
                      <papertitle>Failure-aware Policy Learning for Self-assessable Robotics Tasks</papertitle>
                  </a>
                  <br>
                  <strong>Kechun Xu</strong>,
                  <a href="https://www.rjchen.site/">Runjian Chen</a>,
                  <a>Shuqi Zhao</a>,
                  <a href="https://kyleleey.github.io/">Zizhang Li</a>,
                  <a>Hongxiang Yu</a>,
                  <a href="https://chenci107.github.io/">Ci Chen</a>,
                  <a href="https://ywang-zju.github.io/">Yue Wang</a>,
                  <a href="https://person.zju.edu.cn/en/rongxiong">Rong Xiong</a>
                  <br>
                  <em>IEEE International Conference on Robotics and Automation (ICRA 2023)</em>
                  </p>
                  <div class="paper" id="Xu2023FMP">
                    <a href="https://arxiv.org/abs/2302.13024">arXiv</a> /
                    <a href="https://ieeexplore.ieee.org/document/10160889">ieee</a> /
                    <a href="https://www.bilibili.com/video/BV1Xz4y1V71J/?spm_id_from=333.999.0.0">video</a>
                  </div>
              </td>
          </tr> <!--xu2023fmp-->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/2022_SFN_xie.gif' width="250"></div>
                      </td>
              <td width="75%" valign="middle">
                  <p>
                  <a href="https://arxiv.org/abs/2205.04297">
                      <papertitle>Learning A Simulation-based Visual Policy for Real-world Peg In Unseen Holes</papertitle>
                  </a>
                  <br>
                  <a>Liang Xie</a>,
                  <a>Hongxiang Yu</a>,
                  <strong>Kechun Xu</strong>,
                  <a>Tong Yang</a>,
                  <a>Minhang Wang</a>,
                  <a href="https://person.zju.edu.cn/en/luhaojian">Haojian Lu</a>,
                  <a href="https://person.zju.edu.cn/en/rongxiong">Rong Xiong</a>,
                  <a href="https://ywang-zju.github.io/">Yue Wang</a>
                  <br>
                  <em>Review of Scientific Instruments (RSI 2023)</em>
                  </p>
                  <div class="paper" id="xie2022SFN">
                    <a href="https://arxiv.org/pdf/2205.04297">arXiv</a> /
                    <a href="https://github.com/xieliang555/SFN">code</a>
                  </div>
              </td>
          </tr> 
          <!--xie2022SFN-->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/2022_ENeRV_li.png' width="250"></div>
                      </td>
              <td width="75%" valign="middle">
                  <p>
                  <a href="https://arxiv.org/abs/2207.08132">
                    <papertitle>E-NeRV: Expedite Neural Video Representation with Disentangled Spatial-Temporal Context</papertitle>
                  </a>
                  <br>
                  <a href="https://kyleleey.github.io/">Zizhang Li</a>,
                  <a href="https://sallymmx.github.io/">Mengmeng Wang</a>,
                  <a href="https://phj128.github.io/">Huaijin Pi</a>,
                  <strong>Kechun Xu</strong>,
                  <a href="https://jianbiaomei.github.io/">Jianbiao Mei</a>,
                  <a href="https://person.zju.edu.cn/en/yongliu">Yong Liu</a>
                  <br>
                  <em>European Conference on Computer Vision (ECCV 2022)</em>
                  </p>
                  <div class="paper" id="li2022ENeRV">
                    <a href="https://arxiv.org/abs/2207.08132">arXiv</a> /
                    <a href="https://github.com/kyleleey/E-NeRV">code</a>
                  </div>
              </td>
          </tr>
          <!--li2022enerv-->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/2021EOM_xu.gif' width="250"></div>
                      </td>
              <td width="75%" valign="middle">
                  <p>
                  <a href="https://arxiv.org/abs/2109.10583">
                      <papertitle>Efficient Object Manipulation to an Arbitrary Goal Pose: Learning-based Anytime Prioritized Planning</papertitle>
                  </a>
                  <br>
                  <strong>Kechun Xu</strong>,
                  <a>Hongxiang Yu</a>,
                  <a href="https://renlanghuang.github.io/">Renlang Huang</a>,
                  <a>Dashun Guo</a>,
                  <a href="https://ywang-zju.github.io/">Yue Wang</a>,
                  <a href="https://person.zju.edu.cn/en/rongxiong">Rong Xiong</a>
                  <br>
                  <em>IEEE International Conference on Robotics and Automation (ICRA 2022)</em>
                  </p>
                  <div class="paper" id="Xu2021EOM">
                      <a href="https://arxiv.org/abs/2109.10583">arXiv</a> /
                      <a href="https://ieeexplore.ieee.org/document/9811547/">ieee</a> / 
                      <a href="https://www.bilibili.com/video/BV1Fq4y1P72M?spm_id_from=333.999.0.0">video</a>
                  </div>
              </td>
          </tr> <!--xu2021eom-->
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:20px;width:35%;vertical-align:middle">
                    <img src='images/2021NAE-DF_yu.gif' width="250"></div>
                        </td>
                <td width="75%" valign="middle">
                    <p>
                    <a href="https://arxiv.org/abs/2103.08368">
                        <papertitle>Neural Motion Prediction for In-flight Uneven Object Catching</papertitle>
                    </a>
                    <br>
                    <a>Hongxiang Yu</a>,
                    <a>Dashun Guo</a>,
                    <a href="https://huanyin94.github.io/">Huan Yin</a>,
                    <a>Anzhe Chen</a>,
                    <strong>Kechun Xu</strong>,
                    <a href="https://ywang-zju.github.io/">Yue Wang</a>,
                    <a href="https://person.zju.edu.cn/en/rongxiong">Rong Xiong</a>
                    <br>
                    <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2021)</em>
                    </p>
                    <div class="paper" id="Yu2021NAE-DF">
                        <a href="https://arxiv.org/abs/2103.08368">arXiv</a> /
                        <a href="https://ieeexplore.ieee.org/document/9635983/">ieee</a> / 
                        <a href="https://sites.google.com/view/neural-motion-prediction">project</a> /
                        <a href="https://www.bilibili.com/video/BV1FV411e7Tf?spm_id_from=333.999.0.0">video</a>
                    </div>
                </td>
            </tr> <!--yu2021nae-df-->
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:20px;width:35%;vertical-align:middle">
                    <img src='images/2021PGN_xu.gif' width="250"></div>
                        </td>
                <td width="75%" valign="middle">
                    <p>
                    <a href="https://arxiv.org/abs/2103.05405">
                        <papertitle>Efficient Learning of Goal-oriented Push-grasping Synergy in Clutter</papertitle>
                    </a>
                    <br>
                    <strong>Kechun Xu</strong>,
                    <a>Hongxiang Yu</a>,
                    <a>Qianen Lai</a>,
                    <a href="https://ywang-zju.github.io/">Yue Wang</a>,
                    <a href="https://person.zju.edu.cn/en/rongxiong">Rong Xiong</a>
                    <br>
                    <em>IEEE Robotics and Automation Letter (RA-L with IROS 2021 option)</em>
                    </p>
                    <div class="paper" id="Xu2021EOM">
                        <a href="https://arxiv.org/abs/2103.05405">arXiv</a> /
                        <a href="https://ieeexplore.ieee.org/document/9465702/">ieee</a> /
                        <a href="https://github.com/xukechun/Efficient_goal-oriented_push-grasping_synergy">code</a> /
                        <a href="https://www.bilibili.com/video/BV1sK4y1U7nJ?spm_id_from=333.999.0.0">video</a>
                    </div>
                </td>
            </tr> <!--xu2021pgn-->

          </tbody></table>

          <hr>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                    <heading>Services</heading>
                </td>
            </tr>
          </tbody></table>
  
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <ul>
                <li>Reviewer of <b>T-RO</b>, <b>T-MECH</b>, <b>T-ASE</b>, <b>RA-L</b>, <b>T-CDS</b>, <b>RSS</b>, <b>CoRL</b>, <b>ICRA</b>, <b>IROS</b>, <b>Humanoids</b>.</li>
              </ul>
            </td>
          </tr>
          <tr>
            <ul>
              <li>Executive Committee Member of <a href="https://www.roboarts.cn/index">ARTS</a> 2025.</li>
            </ul>
          </td>
        </tr>
        <tr>
          <ul>
            <li>Volunteer of <a href="https://www.roboarts.cn/index">IROS</a> 2025, and <a href="http://www.cse.zju.edu.cn/2023/0111/c39283a2708625/page.htm">CCIR</a> 2022.</li>
          </ul>
        </td>
      </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                    <heading>Selected Honors and Awards</heading>
                </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <ul>
                <li> Honorable Mention, <a href="https://waymo.com/open/challenges">The Waymo Open Sim Agents Challenge</a>, 2025.</li>
              </ul>
            </td>
          </tr>
          <tr>
            <ul>
              <li> <a href="https://ceai.caai.cn/s">CEAI</a> Outstanding Young Scholar Paper Award Finalist, top10, China, 2025.</li>
            </ul>
          </td>
        </tr>
            <tr>
              <ul>
                <li> <a href="https://www.roboarts.cn/index">Autonomous Robotic Technology Seminar (ARTS) Scholarship</a>, top5, China, 2024.</li>
              </ul>
            </td>
          </tr>
          <tr>
            <ul>
              <li> Finalist, <a href="https://ur.bytedance.com/scholarship">ByteDance Scholarship</a>, top40, China and Singapore, 2024. </li>
            </ul>
          </td>
        </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;">Template from <a href="https://jonbarron.info/">Jon Barron</a></p>
              </td>
            </tr>
          </tbody></table>


      </td>
    </tr>
    <tr style="padding:200px">
      <script type='text/javascript' id='clustrmaps'
        src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=4z8V1gNS4ZaZr3c7zjIyBg_Zqm3bdY0PoDD9m7VD8R0'></script>
    </tr>

    

  </table>
</body>

</html>
