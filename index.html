<!DOCTYPE HTML>
<html lang="en">
  
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Kechun Xu</title>
  
  <meta name="author" content="Kechun Xu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <!-- <link rel="icon" type="image/png" href="images/zju_icon.png"> -->
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Kechun Xu</name>
              </p>
              <p>I am a second-year PhD (Sep. 2021 - ) student in Control Science and Engineering department at <a href="https://www.zju.edu.cn/english/">Zhejiang University</a>. I belong to Robotics Lab</a>, advised by Prof. <a href="https://person.zju.edu.cn/0097062">Rong Xiong</a> and <a href="https://ywang-zju.github.io/">Yue Wang</a>.
              </p>
              <p>
                I obtained my B.Eng (Sep. 2017 - Jun. 2021) in Control Science and Engineering and B.Eng (Sep. 2017 - Jun. 2021) in Mechanical and Electronic Engineering from <a href="https://www.zju.edu.cn/">Zhejiang University</a> with an honor degree at Chu Kochen Honor College.
              </p>
              <p style="text-align:center">
                <a href="mailto:kcxu@zju.edu.cn">Email</a> &nbsp/&nbsp
                <!-- <a href="data/CV.pdf">CV</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?hl=en&user=WWsR278AAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/xukechun">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/kechun-xu-12aaab1a7/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/Kechun_Xu.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Kechun_Xu.jpg" class="hoverZoomLink"></a>
              <p style="text-align:center">
                Photo taken by <a href="https://kyleleey.github.io/">Zizhang Li</a>.
              </p>
            </td>
          </tr>
        </tbody></table>

        <hr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:10px;width:100%;vertical-align:middle">
                  <heading>Recent News</heading>
              </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <ul>
              <li>[06/2023] One paper accepted to <b>RAL 2023</b>.</li>
            </ul>
          </td>
        </tr>
          <tr>
            <ul>
              <li>[04/2023] One paper accepted to <b>ICARM 2023</b>.</li>
            </ul>
          </td>
        </tr>
          <tr>
            <ul>
              <li>[01/2023] Two papers accepted to <b>ICRA 2023</b>.</li>
            </ul>
          </td>
        </tr>
          <tr>
            <ul>
              <li>[07/2022] One paper accepted to <b>ECCV 2022</b>.</li>
            </ul>
          </td>
        </tr>
          <tr>
            <ul>
              <li>[02/2022] One paper accepted to <b>ICRA 2022</b>.</li>
            </ul>
          </td>
        </tr>
          <tr>
              <ul>
                <li>[07/2021] Two papers accepted to <b>IROS 2021</b>.</li>
              </ul>
            </td>
          </tr>
          <tr>
              <ul>
                <li>[06/2021] One paper accepted to <b>RAL 2021</b>.</li>
              </ul>
            </td>
          </tr>
        </tbody></table>
        
        <hr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:10px;width:100%;vertical-align:middle">
            <heading>Publications</heading>
          </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <ul>
            My current research interests lie in robotics learning and manipulation.
          </ul>
          <!-- <ul>
            * indicates equal contributions.
          </ul> -->
        </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/2023HPN-NC_yu.png' width="250"></div>
                      </td>
              <td width="75%" valign="middle">
                  <p>
                  <a href="https://arxiv.org/abs/2304.08952">
                      <papertitle>A Hyper-network Based End-to-end Visual Servoing with Arbitrary Desired Poses</papertitle>
                  </a>
                  <br>
                  <a>Hongxiang Yu</a>,
                  <a>Anzhe Chen</a>,
                  <strong>Kechun Xu</strong>,
                  <a>Zhongxiang Zhou</a>,
                  <a>Wei Jing</a>,
                  <a href="https://ywang-zju.github.io/">Yue Wang</a>,
                  <a href="https://person.zju.edu.cn/0097062">Rong Xiong</a>
                  <br>
                  <em>IEEE Robotics and Automation Letter (RA-L 2023)</em>
                  </p>
                  <div class="paper" id="Yu2023HPNNC">
                    <a href="https://arxiv.org/abs/2304.08952">arXiv</a> /
                    <a href="https://ieeexplore.ieee.org/document/10158789">ieee</a>

                  </div>
              </td>
          </tr> <!--xu2023ocfmlp-->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/2023OCFMLP_xu.png' width="250"></div>
                      </td>
              <td width="75%" valign="middle">
                  <p>
                  <a href="https://arxiv.org/abs/2304.02893">
                      <papertitle>Object-centric Inference for Language Conditioned Placement: A Foundation Model based Approach</papertitle>
                  </a>
                  <br>
                  <a href="https://ariszxxu.github.io/">Zhixuan Xu</a>,
                  <strong>Kechun Xu</strong>,
                  <a href="https://ywang-zju.github.io/">Yue Wang</a>,
                  <a href="https://person.zju.edu.cn/0097062">Rong Xiong</a>
                  <br>
                  <em>IEEE International Conference on Advanced Robotics and Mechatronics (ICARM 2023)</em>
                  </p>
                  <div class="paper" id="Xu2023OCFMLP">
                    <a href="https://arxiv.org/abs/2304.02893">arXiv</a> /
                    <a href="https://ieeexplore.ieee.org/document/10218865/">ieee</a>
                  </div>
              </td>
          </tr> <!--xu2023ocfmlp-->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/2023ViLG_xu.png' width="250"></div>
                      </td>
              <td width="75%" valign="middle">
                  <p>
                  <a href="https://arxiv.org/abs/2302.12610">
                      <papertitle>A Joint Modeling of Vision-Language-Action for Target-oriented Grasping in Clutter</papertitle>
                  </a>
                  <br>
                  <strong>Kechun Xu</strong>,
                  <a>Shuqi Zhao</a>,
                  <a>Zhongxiang Zhou</a>,
                  <a href="https://kyleleey.github.io/">Zizhang Li</a>,
                  <a>Huaijin Pi</a>,
                  <a href="https://zhuyifengzju.github.io/">Yifeng Zhu</a>,
                  <a href="https://ywang-zju.github.io/">Yue Wang</a>,
                  <a href="https://person.zju.edu.cn/0097062">Rong Xiong</a>
                  <br>
                  <em>IEEE International Conference on Robotics and Automation (ICRA 2023)</em>
                  </p>
                  <div class="paper" id="Xu2023ViLG">
                    <a href="https://arxiv.org/abs/2302.12610">arXiv</a> /
                    <a href="https://ieeexplore.ieee.org/document/10161041">ieee</a> /
                    <a href="https://github.com/xukechun/Vision-Language-Grasping">code</a> /
                    <a href="https://www.bilibili.com/video/BV1yh4y1a7Ha/?spm_id_from=333.999.0.0">video</a>
                  </div>
              </td>
          </tr> <!--xu2023vilg-->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/2023FMP_xu.png' width="250"></div>
                      </td>
              <td width="75%" valign="middle">
                  <p>
                    <a href="https://arxiv.org/abs/2302.13024">
                      <papertitle>Failure-aware Policy Learning for Self-assessable Robotics Tasks</papertitle>
                  </a>
                  <br>
                  <strong>Kechun Xu</strong>,
                  <a href="https://www.rjchen.site/">Runjian Chen</a>,
                  <a>Shuqi Zhao</a>,
                  <a href="https://kyleleey.github.io/">Zizhang Li</a>,
                  <a>Hongxiang Yu</a>,
                  <a>Ci Chen</a>,
                  <a href="https://ywang-zju.github.io/">Yue Wang</a>,
                  <a href="https://person.zju.edu.cn/0097062">Rong Xiong</a>
                  <br>
                  <em>IEEE International Conference on Robotics and Automation (ICRA 2023)</em>
                  </p>
                  <div class="paper" id="Xu2023FMP">
                    <a href="https://arxiv.org/abs/2302.13024">arXiv</a> /
                    <a href="https://ieeexplore.ieee.org/document/10160889">ieee</a> /
                    <a href="https://www.bilibili.com/video/BV1Xz4y1V71J/?spm_id_from=333.999.0.0">video</a>
                  </div>
              </td>
          </tr> <!--xu2023fmp-->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/2022_SFN_xie.png' width="250"></div>
                      </td>
              <td width="75%" valign="middle">
                  <p>
                  <a href="https://arxiv.org/abs/2205.04297">
                      <papertitle>Learning A Simulation-based Visual Policy for Real-world Peg In Unseen Holes</papertitle>
                  </a>
                  <br>
                  <a>Liang Xie</a>,
                  <a>Hongxiang Yu</a>,
                  <strong>Kechun Xu</strong>,
                  <a>Tong Yang</a>,
                  <a>Minhang Wang</a>,
                  <a>Haojian Lu</a>,
                  <a href="https://person.zju.edu.cn/0097062">Rong Xiong</a>,
                  <a href="https://ywang-zju.github.io/">Yue Wang</a>
                  <br>
                  <em>The Review of scientific instruments</em>
                  </p>
                  <div class="paper" id="xie2022SFN">
                    <a href="https://arxiv.org/pdf/2205.04297">arXiv</a> /
                    <a href="https://github.com/xieliang555/SFN">code</a>
                  </div>
              </td>
          </tr> 
          <!--xie2022SFN-->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/2022_ENeRV_li.png' width="250"></div>
                      </td>
              <td width="75%" valign="middle">
                  <p>
                  <a href="https://arxiv.org/abs/2207.08132">
                    <papertitle>E-NeRV: Expedite Neural Video Representation with Disentangled Spatial-Temporal Context</papertitle>
                  </a>
                  <br>
                  <a href="https://kyleleey.github.io/">Zizhang Li</a>,
                  <a href="https://sallymmx.github.io/">Mengmeng Wang</a>,
                  <a>Huaijin Pi</a>,
                  <strong>Kechun Xu</strong>,
                  <a>Jianbiao Mei</a>,
                  <a href="https://person.zju.edu.cn/yongliu">Yong Liu</a>
                  <br>
                  <em>European Conference on Computer Vision (ECCV 2022)</em>
                  </p>
                  <div class="paper" id="li2022ENeRV">
                    <a href="https://arxiv.org/abs/2207.08132">arXiv</a> /
                    <a href="https://github.com/kyleleey/E-NeRV">code</a>
                  </div>
              </td>
          </tr>
          <!--li2022enerv-->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:20px;width:35%;vertical-align:middle">
                  <img src='images/2021EOM_xu.png' width="250"></div>
                      </td>
              <td width="75%" valign="middle">
                  <p>
                  <a href="https://arxiv.org/abs/2109.10583">
                      <papertitle>Efficient Object Manipulation to an Arbitrary Goal Pose: Learning-based Anytime Prioritized Planning</papertitle>
                  </a>
                  <br>
                  <strong>Kechun Xu</strong>,
                  <a>Hongxiang Yu</a>,
                  <a>Renlang Huang</a>,
                  <a>Dashun Guo</a>,
                  <a href="https://ywang-zju.github.io/">Yue Wang</a>,
                  <a href="https://person.zju.edu.cn/0097062">Rong Xiong</a>
                  <br>
                  <em>IEEE International Conference on Robotics and Automation (ICRA 2022)</em>
                  </p>
                  <div class="paper" id="Xu2021EOM">
                      <a href="https://arxiv.org/abs/2109.10583">arXiv</a> /
                      <a href="https://ieeexplore.ieee.org/document/9811547/">ieee</a> / 
                      <a href="https://www.bilibili.com/video/BV1Fq4y1P72M?spm_id_from=333.999.0.0">video</a>
                  </div>
              </td>
          </tr> <!--xu2021eom-->
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:20px;width:35%;vertical-align:middle">
                    <img src='images/2021NAE-DF_yu.png' width="250"></div>
                        </td>
                <td width="75%" valign="middle">
                    <p>
                    <a href="https://arxiv.org/abs/2103.08368">
                        <papertitle>Neural motion prediction for in-flight uneven object catching</papertitle>
                    </a>
                    <br>
                    <a>Hongxiang Yu</a>,
                    <a>Dashun Guo</a>,
                    <a>Huan Yin</a>,
                    <a>Anzhe Chen</a>,
                    <strong>Kechun Xu</strong>,
                    <a href="https://ywang-zju.github.io/">Yue Wang</a>,
                    <a href="https://person.zju.edu.cn/0097062">Rong Xiong</a>
                    <br>
                    <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2021)</em>
                    </p>
                    <div class="paper" id="Yu2021NAE-DF">
                        <a href="https://arxiv.org/abs/2103.08368">arXiv</a> /
                        <a href="https://ieeexplore.ieee.org/document/9635983/">ieee</a> / 
                        <a href="https://sites.google.com/view/neural-motion-prediction">project</a> /
                        <a href="https://www.bilibili.com/video/BV1FV411e7Tf?spm_id_from=333.999.0.0">video</a>
                    </div>
                </td>
            </tr> <!--yu2021nae-df-->
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:20px;width:35%;vertical-align:middle">
                    <img src='images/2021PGN_xu.png' width="250"></div>
                        </td>
                <td width="75%" valign="middle">
                    <p>
                    <a href="https://arxiv.org/abs/2103.05405">
                        <papertitle>Efficient learning of goal-oriented push-grasping synergy in clutter</papertitle>
                    </a>
                    <br>
                    <strong>Kechun Xu</strong>,
                    <a>Hongxiang Yu</a>,
                    <a>Qianen Lai</a>,
                    <a href="https://ywang-zju.github.io/">Yue Wang</a>,
                    <a href="https://person.zju.edu.cn/0097062">Rong Xiong</a>
                    <br>
                    <em>IEEE Robotics and Automation Letter (RA-L with IROS 2021 option)</em>
                    </p>
                    <div class="paper" id="Xu2021EOM">
                        <a href="https://arxiv.org/abs/2103.05405">arXiv</a> /
                        <a href="https://ieeexplore.ieee.org/document/9465702/">ieee</a> /
                        <a href="https://github.com/xukechun/Efficient_goal-oriented_push-grasping_synergy">code</a> /
                        <a href="https://www.bilibili.com/video/BV1sK4y1U7nJ?spm_id_from=333.999.0.0">video</a>
                    </div>
                </td>
            </tr> <!--xu2021pgn-->

          </tbody></table>

          <hr>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                    <heading>Services</heading>
                </td>
            </tr>
          </tbody></table>
  
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <ul>
                <li>Reviewer of <b>ICRA</b>, <b>IROS</b>, <b>RA-L</b>.</li>
              </ul>
            </td>
          </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;">Template from <a href="https://jonbarron.info/">Jon Barron</a></p>
              </td>
            </tr>
          </tbody></table>
        
      </td>
    </tr>
    <tr style="padding:200px">
      <script type='text/javascript' id='clustrmaps'
        src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=4z8V1gNS4ZaZr3c7zjIyBg_Zqm3bdY0PoDD9m7VD8R0'></script>
    </tr>

    

  </table>
</body>

</html>
